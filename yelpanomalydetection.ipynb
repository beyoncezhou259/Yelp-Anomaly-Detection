{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Set of Inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"insertapikey\"\n",
    "client = OpenAI()\n",
    "\n",
    "file_review = \"yelp_academic_dataset_review.json\"\n",
    "reviews_list = []\n",
    "sample_size = 200\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "with open(file_review, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = json.loads(line)\n",
    "        obj['_line_number'] = i + 1\n",
    "        if len(reviews_list) < sample_size:\n",
    "            reviews_list.append(obj)\n",
    "        else:\n",
    "            # Reservoir sampling: replace with decreasing probability\n",
    "            j = random.randint(0, i)\n",
    "            if j < sample_size:\n",
    "                reviews_list[j] = obj\n",
    "\n",
    "reviews = pd.DataFrame(reviews_list)\n",
    "sample_reviews = reviews['text'].tolist()\n",
    "original_stars = [int(s) for s in reviews['stars']]\n",
    "\n",
    "#3. prompts\n",
    "prompt_discrete = (\n",
    "    \"You are a Yelp user. Given a Yelp review, predict the star rating (1 to 5). \"\n",
    "    \"Only respond with a number (1 to 5).\"\n",
    ")\n",
    "prompt_probs = (\n",
    "    \"You are a Yelp user. Given a Yelp review, estimate the probability the user gave each star rating from 1 to 5.\\n\"\n",
    "    \"Output five numbers between 0 and 1 that sum to 1, in order: P(1), P(2), P(3), P(4), P(5). \"\n",
    "    \"Only output the numbers, separated by spaces. No labels.\"\n",
    ")\n",
    "\n",
    "# Single prediction loop\n",
    "discrete_preds = []\n",
    "prob_preds = []\n",
    "\n",
    "print(\"\\n--- Running predictions for 200 reviews ---\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"Review {i+1}/200\")\n",
    "\n",
    "    # Discrete prediction\n",
    "    try:\n",
    "        res_discrete = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_discrete},\n",
    "                {\"role\": \"user\", \"content\": review}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        pred = int(res_discrete.choices[0].message.content.strip())\n",
    "    except:\n",
    "        pred = np.nan\n",
    "    discrete_preds.append(pred)\n",
    "\n",
    "    # Probability prediction\n",
    "    try:\n",
    "        res_probs = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_probs},\n",
    "                {\"role\": \"user\", \"content\": review}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        probs = list(map(float, res_probs.choices[0].message.content.strip().split()))\n",
    "        if len(probs) != 5:\n",
    "            raise Exception(\"Incorrect prob format\")\n",
    "    except:\n",
    "        probs = [np.nan] * 5\n",
    "    prob_preds.append(probs)\n",
    "\n",
    "# Compute inconsistency scores\n",
    "inconsistency_discrete = []\n",
    "inconsistency_prob = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    true = original_stars[i]\n",
    "    pred_d = discrete_preds[i]\n",
    "    pred_p = prob_preds[i]\n",
    "\n",
    "    # Discrete inconsistency\n",
    "    if not np.isnan(pred_d):\n",
    "        inconsistency_discrete.append(abs(pred_d - true))\n",
    "    else:\n",
    "        inconsistency_discrete.append(np.nan)\n",
    "\n",
    "    # Probabilistic inconsistency: -log(P(true label))\n",
    "    if not np.isnan(pred_p).any():\n",
    "        log_prob = -np.log(pred_p[true - 1] + 1e-12)\n",
    "        inconsistency_prob.append(log_prob)\n",
    "    else:\n",
    "        inconsistency_prob.append(np.nan)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    \"Review #\": range(1, sample_size + 1),\n",
    "    \"True Rating\": original_stars,\n",
    "    \"Discrete Prediction\": discrete_preds,\n",
    "    \"Discrete Inconsistency (|pred - true|)\": inconsistency_discrete,\n",
    "    \"Prob Vector\": prob_preds,\n",
    "    \"Prob Inconsistency (-log(P(true)))\": inconsistency_prob\n",
    "})\n",
    "\n",
    "# Preview first 10 rows\n",
    "print(\"\\n--- Inconsistency Scores Side by Side (First 10 Reviews) ---\")\n",
    "print(output_df.head(10).to_string(index=False))\n",
    "\n",
    "# Filter out NaNs\n",
    "valid_discrete_inconsistencies = [v for v in inconsistency_discrete if not np.isnan(v)]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(valid_discrete_inconsistencies, bins=range(0, 6), align='left', color='skyblue', edgecolor='black', rwidth=0.8)\n",
    "plt.xticks(range(0, 6))\n",
    "plt.xlabel('|Predicted - True| (Discrete Inconsistency)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Distribution of Discrete Inconsistency Scores')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filter out NaNs\n",
    "valid_prob_inconsistencies = [v for v in inconsistency_prob if not np.isnan(v)]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(valid_prob_inconsistencies, bins=20, color='salmon', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('-log(P(True Label)) (Probabilistic Inconsistency)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Distribution of Probabilistic Inconsistency Scores')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Reviews with Discrete Inconsistency â‰¥ 1 ---\")\n",
    "\n",
    "for i in range(sample_size):\n",
    "    if not np.isnan(inconsistency_discrete[i]) and inconsistency_discrete[i] >= 1:\n",
    "        print(f\"\\nReview #{i+1} | True: {original_stars[i]} | Predicted: {discrete_preds[i]} | Inconsistency: {inconsistency_discrete[i]}\")\n",
    "        print(\"Text:\")\n",
    "        print(sample_reviews[i])\n",
    "\n",
    "        \n",
    "# Prepare DataFrame for probabilistic inconsistency\n",
    "prob_df = pd.DataFrame({\n",
    "    \"Review #\": range(1, sample_size + 1),\n",
    "    \"Inconsistency\": inconsistency_prob,\n",
    "    \"Text\": sample_reviews,\n",
    "    \"True Rating\": original_stars,\n",
    "    \"Prob Vector\": prob_preds\n",
    "})\n",
    "\n",
    "# Drop NaNs and get top 10%\n",
    "prob_df = prob_df.dropna(subset=[\"Inconsistency\"])\n",
    "top_k = int(sample_size * 0.10)\n",
    "top_prob_outliers = prob_df.nlargest(top_k, \"Inconsistency\")\n",
    "\n",
    "print(\"\\n--- Top 10% Reviews with Highest Probabilistic Inconsistency ---\")\n",
    "for _, row in top_prob_outliers.iterrows():\n",
    "    print(f\"\\nReview #{int(row['Review #'])} | True: {row['True Rating']} | -log(P(true)): {row['Inconsistency']:.3f}\")\n",
    "    print(\"Probabilities:\", [\"{:.2f}\".format(p) for p in row[\"Prob Vector\"]])\n",
    "    print(\"Text:\")\n",
    "    print(row[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Probability Vector Inlier Dataset and appending Overrated and Underrated Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inlier Dataset: Probabilistic Predictions ---\n",
    "prob_exclude_review_nums = [\n",
    "    135,38,162,12,16,21,25,60,65,91,\n",
    "    101,140,156,26,84,130,185\n",
    "]\n",
    "\n",
    "df_prob_inliers = output_df[\n",
    "    ~output_df[\"Review #\"].isin(prob_exclude_review_nums)\n",
    "][[\n",
    "    \"Review #\", \"True Rating\", \"Prob Vector\", \"Prob Inconsistency (-log(P(true)))\"\n",
    "]]\n",
    "\n",
    "# Add review text\n",
    "df_prob_inliers[\"Review Text\"] = df_prob_inliers[\"Review #\"].apply(lambda x: sample_reviews[x - 1])\n",
    "\n",
    "# Compute expected ratings\n",
    "expected_ratings = df_prob_inliers[\"Prob Vector\"].apply(lambda probs: sum((i+1)*p for i, p in enumerate(probs)))\n",
    "\n",
    "# Compare to true rating\n",
    "df_prob_inliers[\"Expected Rating\"] = expected_ratings\n",
    "\n",
    "# Define overrated/underrated\n",
    "df_prob_inliers[\"Overrated\"] = df_prob_inliers.apply(\n",
    "    lambda row: row[\"Prob Inconsistency (-log(P(true)))\"] if row[\"Expected Rating\"] < row[\"True Rating\"] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_prob_inliers[\"Underrated\"] = df_prob_inliers.apply(\n",
    "    lambda row: row[\"Prob Inconsistency (-log(P(true)))\"] if row[\"Expected Rating\"] > row[\"True Rating\"] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "df_prob_inliers = df_prob_inliers[[\n",
    "    \"Review #\", \"Review Text\", \"True Rating\", \"Prob Vector\", \"Expected Rating\", \"Prob Inconsistency (-log(P(true)))\",\"Overrated\",\"Underrated\"\n",
    "]]\n",
    "\n",
    "print(\"\\n--- Probabilistic Inlier Dataset (Preview) ---\")\n",
    "print(df_prob_inliers.head())\n",
    "\n",
    "df_prob_inliers.to_csv(\"inliers_probabilistic.csv\", index=False)\n",
    "print(\"Inlier datasets saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running for 10,000 Reviews and Applying Benjamini Hochberg w/ Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import ast\n",
    "from scipy.stats import percentileofscore\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from threading import Lock\n",
    "import traceback\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 2  # Adjust based on your OpenAI rate limits\n",
    "BATCH_SIZE = 50  # Number of reviews to process in each batch for progress updates\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"insertapikey\" \n",
    "client = OpenAI()\n",
    "\n",
    "# Thread-safe progress tracking\n",
    "progress_lock = Lock()\n",
    "completed_requests = 0\n",
    "\n",
    "def update_progress(total):\n",
    "    global completed_requests\n",
    "    with progress_lock:\n",
    "        completed_requests += 1\n",
    "        if completed_requests % BATCH_SIZE == 0 or completed_requests == total:\n",
    "            print(f\"Completed {completed_requests}/{total} reviews ({completed_requests/total*100:.1f}%)\")\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    return int(len(text.split()) * 1.5) + 50  # rough estimate\n",
    "\n",
    "def get_probability_prediction(review_data, max_retries=6):\n",
    "    \"\"\"\n",
    "    Process a single review and return probability predictions with retry and throttling.\n",
    "    review_data is a tuple of (index, review_text)\n",
    "    \"\"\"\n",
    "    index, review_text = review_data\n",
    "\n",
    "    prompt_probs = (\n",
    "        \"You are a Yelp user. Given a Yelp review, estimate the probability the user gave each star rating from 1 to 5.\\n\"\n",
    "        \"Output five numbers between 0 and 1 that sum to 1, in order: P(1), P(2), P(3), P(4), P(5). \"\n",
    "        \"Only output the numbers, separated by spaces. No labels.\"\n",
    "    )\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            res_probs = client.chat.completions.create(\n",
    "                model=\"gpt-4.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt_probs},\n",
    "                    {\"role\": \"user\", \"content\": review_text}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=50  # Limit tokens since we only need 5 numbers\n",
    "            )\n",
    "\n",
    "            content = res_probs.choices[0].message.content.strip()\n",
    "            probs = list(map(float, content.split()))\n",
    "\n",
    "            if len(probs) != 5:\n",
    "                raise Exception(\"Incorrect prob format\")\n",
    "\n",
    "            prob_sum = sum(probs)\n",
    "            if prob_sum > 0:\n",
    "                probs = [p / prob_sum for p in probs]\n",
    "            else:\n",
    "                raise Exception(\"All probabilities are zero\")\n",
    "\n",
    "            # Delay based on token estimation to avoid TPM overflow\n",
    "            total_tokens = estimate_tokens(review_text) + 50\n",
    "            delay = total_tokens / 30000 * 60  # 30k TPM max => delay in seconds\n",
    "            time.sleep(delay)\n",
    "\n",
    "            return index, probs\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rate limit case: retry with backoff\n",
    "            error_msg = str(e)\n",
    "            if 'rate limit' in error_msg.lower() or '429' in error_msg:\n",
    "                wait = (2 ** attempt) + random.uniform(0, 2.0)\n",
    "                print(f\"Rate limit hit for review {index}. Retrying in {wait:.2f}s (attempt {attempt + 1})\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"Error processing review {index}: {e}\")\n",
    "                break\n",
    "\n",
    "    return index, [np.nan] * 5\n",
    "\n",
    "\n",
    "def process_reviews_parallel(reviews_text, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    Process reviews in parallel using ThreadPoolExecutor\n",
    "    \"\"\"\n",
    "    global completed_requests\n",
    "    completed_requests = 0\n",
    "    \n",
    "    # Create list of (index, review_text) tuples\n",
    "    review_data = [(i, review) for i, review in enumerate(reviews_text)]\n",
    "    \n",
    "    # Initialize results array\n",
    "    prob_preds = [[np.nan] * 5] * len(reviews_text)\n",
    "    \n",
    "    print(f\"\\n--- Running predictions for {len(reviews_text)} reviews with {max_workers} workers ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_index = {\n",
    "            executor.submit(get_probability_prediction, data): data[0] \n",
    "            for data in review_data\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_index):\n",
    "            try:\n",
    "                index, probs = future.result()\n",
    "                prob_preds[index] = probs\n",
    "                update_progress(len(reviews_text))\n",
    "            except Exception as e:\n",
    "                index = future_to_index[future]\n",
    "                print(f\"Error with review {index}: {e}\")\n",
    "                prob_preds[index] = [np.nan] * 5\n",
    "                update_progress(len(reviews_text))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nCompleted all predictions in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Average time per review: {(end_time - start_time) / len(reviews_text):.3f} seconds\")\n",
    "    \n",
    "    return prob_preds\n",
    "\n",
    "# Load and sample data\n",
    "print(\"Loading and sampling Yelp reviews...\")\n",
    "file_review = \"yelp_academic_dataset_review.json\"\n",
    "reviews_list = []\n",
    "sample_size = 10000\n",
    "random.seed(29)  # for reproducibility\n",
    "\n",
    "with open(file_review, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = json.loads(line)\n",
    "        obj['_line_number'] = i + 1\n",
    "        if len(reviews_list) < sample_size:\n",
    "            reviews_list.append(obj)\n",
    "        else:\n",
    "            # Reservoir sampling: replace with decreasing probability\n",
    "            j = random.randint(0, i)\n",
    "            if j < sample_size:\n",
    "                reviews_list[j] = obj\n",
    "\n",
    "reviews = pd.DataFrame(reviews_list)\n",
    "sample_reviews = reviews['text'].tolist()\n",
    "original_stars = [int(s) for s in reviews['stars']]\n",
    "\n",
    "print(f\"Loaded {len(sample_reviews)} reviews\")\n",
    "\n",
    "# Process reviews in parallel\n",
    "prob_preds = process_reviews_parallel(sample_reviews, max_workers=MAX_WORKERS)\n",
    "\n",
    "# Compute inconsistency scores\n",
    "print(\"\\nComputing inconsistency scores...\")\n",
    "inconsistency_prob = []\n",
    "\n",
    "for i in range(sample_size):\n",
    "    true = original_stars[i]\n",
    "    pred_p = prob_preds[i]\n",
    "\n",
    "    # Probabilistic inconsistency: -log(P(true label))\n",
    "    if not np.isnan(pred_p).any():\n",
    "        log_prob = -np.log(pred_p[true - 1] + 1e-12)\n",
    "        inconsistency_prob.append(log_prob)\n",
    "    else:\n",
    "        inconsistency_prob.append(np.nan)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    \"Review #\": range(1, sample_size + 1),\n",
    "    \"True Rating\": original_stars,\n",
    "    \"Prob Vector\": prob_preds,\n",
    "    \"Prob Inconsistency (-log(P(true)))\": inconsistency_prob\n",
    "})\n",
    "\n",
    "# Preview first 10 rows\n",
    "print(\"\\n--- Inconsistency Scores Side by Side (First 10 Reviews) ---\")\n",
    "print(output_df.head(10).to_string(index=False))\n",
    "\n",
    "# Filter out NaNs for statistics\n",
    "valid_prob_inconsistencies = [v for v in inconsistency_prob if not np.isnan(v)]\n",
    "nan_count = len(inconsistency_prob) - len(valid_prob_inconsistencies)\n",
    "\n",
    "print(f\"\\nProcessing Statistics:\")\n",
    "print(f\"Total reviews: {len(inconsistency_prob)}\")\n",
    "print(f\"Valid predictions: {len(valid_prob_inconsistencies)}\")\n",
    "print(f\"Failed predictions: {nan_count}\")\n",
    "print(f\"Success rate: {len(valid_prob_inconsistencies)/len(inconsistency_prob)*100:.1f}%\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(valid_prob_inconsistencies, bins=30, color='salmon', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('-log(P(True Label)) (Probabilistic Inconsistency)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Distribution of Probabilistic Inconsistency Scores')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare DataFrame for probabilistic inconsistency\n",
    "prob_df = pd.DataFrame({\n",
    "    \"Review #\": range(1, sample_size + 1),\n",
    "    \"Inconsistency\": inconsistency_prob,\n",
    "    \"Text\": sample_reviews,\n",
    "    \"True Rating\": original_stars,\n",
    "    \"Prob Vector\": prob_preds\n",
    "})\n",
    "\n",
    "# Drop NaNs and get top 10%\n",
    "prob_df = prob_df.dropna(subset=[\"Inconsistency\"])\n",
    "top_k = int(len(prob_df) * 0.10)  # Use actual valid count, not original sample_size\n",
    "top_prob_outliers = prob_df.nlargest(top_k, \"Inconsistency\")\n",
    "\n",
    "print(f\"\\n--- Top 10% Reviews with Highest Probabilistic Inconsistency (Top {top_k}) ---\")\n",
    "for _, row in top_prob_outliers.head(5).iterrows():  # Show only first 5 for brevity\n",
    "    print(f\"\\nReview #{int(row['Review #'])} | True: {row['True Rating']} | -log(P(true)): {row['Inconsistency']:.3f}\")\n",
    "    print(\"Probabilities:\", [\"{:.2f}\".format(p) for p in row[\"Prob Vector\"]])\n",
    "    print(\"Text:\")\n",
    "    print(row[\"Text\"][:200] + \"...\" if len(row[\"Text\"]) > 200 else row[\"Text\"])\n",
    "\n",
    "# Load inlier dataset (if it exists)\n",
    "try:\n",
    "    inliers_prob = pd.read_csv(\"inliers_probabilistic.csv\")\n",
    "    inliers_prob[\"Prob Vector\"] = inliers_prob[\"Prob Vector\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Separate into overrated and underrated inliers\n",
    "    overrated_inliers = inliers_prob[inliers_prob[\"Expected Rating\"] <= inliers_prob[\"True Rating\"]]\n",
    "    underrated_inliers = inliers_prob[inliers_prob[\"Expected Rating\"] >= inliers_prob[\"True Rating\"]]\n",
    "\n",
    "    overrated_scores_inlier = overrated_inliers[\"Overrated\"].values\n",
    "    underrated_scores_inlier = underrated_inliers[\"Underrated\"].values\n",
    "\n",
    "    print(f\"\\nLoaded {len(inliers_prob)} inlier reviews for comparison\")\n",
    "    \n",
    "    # Compute expected rating and scores for 10000 reviews\n",
    "    print(\"Computing directional inconsistency scores...\")\n",
    "    expected_ratings = []\n",
    "    overrated_scores_10000 = []\n",
    "    underrated_scores_10000 = []\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        probs = prob_preds[i]\n",
    "        true = original_stars[i]\n",
    "\n",
    "        if np.isnan(probs).any():\n",
    "            expected_ratings.append(np.nan)\n",
    "            overrated_scores_10000.append(np.nan)\n",
    "            underrated_scores_10000.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        expected = sum((j + 1) * p for j, p in enumerate(probs))\n",
    "        inconsistency = -np.log(probs[true - 1] + 1e-12)\n",
    "\n",
    "        expected_ratings.append(expected)\n",
    "        if expected < true:\n",
    "            overrated_scores_10000.append(inconsistency)\n",
    "            underrated_scores_10000.append(0)\n",
    "        elif expected > true:\n",
    "            overrated_scores_10000.append(0)\n",
    "            underrated_scores_10000.append(inconsistency)\n",
    "        else:\n",
    "            overrated_scores_10000.append(0)\n",
    "            underrated_scores_10000.append(0)\n",
    "\n",
    "    output_df[\"Expected Rating\"] = expected_ratings\n",
    "    output_df[\"Overrated Score\"] = overrated_scores_10000\n",
    "    output_df[\"Underrated Score\"] = underrated_scores_10000\n",
    "\n",
    "    # Compute directional p-values\n",
    "    def get_pvals(scores, inlier_scores):\n",
    "        return [\n",
    "            1 - percentileofscore(inlier_scores, score, kind='mean') / 100 if score > 0 else 1.0\n",
    "            for score in scores\n",
    "        ]\n",
    "\n",
    "    print(\"Computing p-values...\")\n",
    "    pvals_overrated = get_pvals(overrated_scores_10000, overrated_scores_inlier)\n",
    "    pvals_underrated = get_pvals(underrated_scores_10000, underrated_scores_inlier)\n",
    "\n",
    "    # Apply Benjamini-Hochberg (FDR)\n",
    "    print(\"Applying FDR correction...\")\n",
    "    rejected_over, pvals_over_adj, _, _ = multipletests(pvals_overrated, alpha=0.2, method='fdr_bh')\n",
    "    rejected_under, pvals_under_adj, _, _ = multipletests(pvals_underrated, alpha=0.2, method='fdr_bh')\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    output_df[\"Overrated P-Value\"] = pvals_overrated\n",
    "    output_df[\"Overrated P-Adj\"] = pvals_over_adj\n",
    "    output_df[\"Is Overrated Outlier\"] = rejected_over\n",
    "\n",
    "    output_df[\"Underrated P-Value\"] = pvals_underrated\n",
    "    output_df[\"Underrated P-Adj\"] = pvals_under_adj\n",
    "    output_df[\"Is Underrated Outlier\"] = rejected_under\n",
    "    \n",
    "    print(f\"Found {sum(rejected_over)} overrated outliers and {sum(rejected_under)} underrated outliers\")\n",
    "    \n",
    "    # Preview outliers\n",
    "    print(\"\\n--- OVERRATED OUTLIERS ---\")\n",
    "    overrated_outliers = output_df[output_df[\"Is Overrated Outlier\"]]\n",
    "    if len(overrated_outliers) > 0:\n",
    "        print(overrated_outliers[[\"Review #\", \"True Rating\", \"Expected Rating\", \"Overrated Score\", \"Overrated P-Adj\"]].head())\n",
    "    else:\n",
    "        print(\"No overrated outliers found\")\n",
    "\n",
    "    print(\"\\n--- UNDERRATED OUTLIERS ---\")\n",
    "    underrated_outliers = output_df[output_df[\"Is Underrated Outlier\"]]\n",
    "    if len(underrated_outliers) > 0:\n",
    "        print(underrated_outliers[[\"Review #\", \"True Rating\", \"Expected Rating\", \"Underrated Score\", \"Underrated P-Adj\"]].head())\n",
    "    else:\n",
    "        print(\"No underrated outliers found\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nWarning: 'inliers_probabilistic.csv' not found. Skipping directional analysis.\")\n",
    "    print(\"Only basic inconsistency analysis will be performed.\")\n",
    "\n",
    "# Add review text to output\n",
    "output_df[\"Review Text\"] = sample_reviews\n",
    "\n",
    "# Save to file\n",
    "output_filename = \"10000_reviews_with_directional_pvals.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nSaved results to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp, spearmanr, percentileofscore\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(\"10000_reviews_with_directional_pvals.csv\")\n",
    "df[\"Prob Vector\"] = df[\"Prob Vector\"].apply(eval)\n",
    "\n",
    "# Compute expected ratings if not in file\n",
    "if \"Expected Rating\" not in df.columns:\n",
    "    df[\"Expected Rating\"] = df[\"Prob Vector\"].apply(lambda probs: sum((i + 1) * p for i, p in enumerate(probs)))\n",
    "\n",
    "# === Q1: ChatGPT vs Human Rating Distributions ===\n",
    "\n",
    "# Prepare data in long format\n",
    "plot_df = pd.DataFrame({\n",
    "    \"Rating\": pd.concat([df[\"True Rating\"], df[\"Expected Rating\"]], ignore_index=True),\n",
    "    \"Source\": [\"Human\"] * len(df) + [\"ChatGPT\"] * len(df)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    data=plot_df,\n",
    "    x=\"Rating\",\n",
    "    hue=\"Source\",\n",
    "    bins=np.arange(1, 7) - 0.5,\n",
    "    multiple=\"dodge\",  # <-- This enables side-by-side bars\n",
    "    shrink=0.9,\n",
    "    palette={\"Human\": \"skyblue\", \"ChatGPT\": \"salmon\"},\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Human vs ChatGPT Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(range(1, 6))\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "ks_stat, ks_pval = ks_2samp(df[\"True Rating\"], df[\"Expected Rating\"])\n",
    "print(\"\\nQ1: ChatGPT vs Human Rating Stats\")\n",
    "print(f\"Mean (Human): {df['True Rating'].mean():.2f}\")\n",
    "print(f\"Mean (ChatGPT): {df['Expected Rating'].mean():.2f}\")\n",
    "print(f\"Std (Human): {df['True Rating'].std():.2f}\")\n",
    "print(f\"Std (ChatGPT): {df['Expected Rating'].std():.2f}\")\n",
    "print(f\"KS Test Statistic: {ks_stat:.3f}, p-value: {ks_pval:.4f}\")\n",
    "\n",
    "# === Q2: Distribution of Inconsistency Scores ===\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"Prob Inconsistency (-log(P(true)))\"].dropna(), bins=30, color='mediumpurple', edgecolor='black')\n",
    "plt.title(\"Distribution of Probabilistic Inconsistency Scores\")\n",
    "plt.xlabel(\"-log(P(True Label))\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQ2: Inconsistency Score Summary\")\n",
    "print(df[\"Prob Inconsistency (-log(P(true)))\"].describe())\n",
    "\n",
    "# Filter reviews with extreme inconsistency scores\n",
    "high_inconsistency_df = df[df[\"Prob Inconsistency (-log(P(true)))\"] > 25]\n",
    "\n",
    "# Sort by score descending (optional)\n",
    "high_inconsistency_df = high_inconsistency_df.sort_values(\"Prob Inconsistency (-log(P(true)))\", ascending=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nFound {len(high_inconsistency_df)} reviews with -log(P(true)) > 25\\n\")\n",
    "\n",
    "for idx, row in high_inconsistency_df.iterrows():\n",
    "    print(f\"--- Review #{int(row['Review #'])} ---\")\n",
    "    print(f\"Human Rating: {row['True Rating']}\")\n",
    "    print(f\"ChatGPT Expected Rating: {row['Expected Rating']:.2f}\")\n",
    "    print(f\"-log(P(true)): {row['Prob Inconsistency (-log(P(true)))']:.4f}\")\n",
    "    print(\"\\nReview Text:\")\n",
    "    print(row[\"Review Text\"])\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# === Q3: Inconsistency Rates by True Rating ===\n",
    "\n",
    "grouped = df.groupby(\"True Rating\")[\"Prob Inconsistency (-log(P(true)))\"]\n",
    "means = grouped.mean()\n",
    "counts = grouped.count()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=means.index, y=means.values, palette=\"YlOrRd\")\n",
    "plt.title(\"Avg. Inconsistency Score by Human Rating\")\n",
    "plt.xlabel(\"True Rating\")\n",
    "plt.ylabel(\"Avg. -log(P(True))\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQ3: Mean Inconsistency by Rating\")\n",
    "print(means)\n",
    "\n",
    "# === Q4: Review Length vs Inconsistency ===\n",
    "\n",
    "df[\"Review Length\"] = df[\"Review Text\"].str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x=\"Review Length\", y=\"Prob Inconsistency (-log(P(true)))\", alpha=0.4)\n",
    "plt.title(\"Review Length vs Inconsistency Score\")\n",
    "plt.xlabel(\"Review Length (word count)\")\n",
    "plt.ylabel(\"-log(P(True Label))\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "corr, pval = spearmanr(df[\"Review Length\"], df[\"Prob Inconsistency (-log(P(true)))\"])\n",
    "print(\"\\nQ4: Length vs Inconsistency\")\n",
    "print(f\"Spearman Correlation: {corr:.3f}, p-value: {pval:.4f}\")\n",
    "\n",
    "# === Q5: Directional Outliers Overview ===\n",
    "\n",
    "# Count overrated and underrated\n",
    "over_count = df[\"Is Overrated Outlier\"].sum()\n",
    "under_count = df[\"Is Underrated Outlier\"].sum()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=[\"Overrated\", \"Underrated\"], y=[over_count, under_count], palette=\"coolwarm\")\n",
    "plt.title(\"Count of Directional Outliers\")\n",
    "plt.ylabel(\"Number of Outliers\")\n",
    "plt.grid(True, axis='y', linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQ5: Directional Outlier Summary\")\n",
    "print(f\"Overrated Outliers: {over_count}\")\n",
    "print(f\"Underrated Outliers: {under_count}\")\n",
    "\n",
    "# Optional: Preview some examples\n",
    "print(\"\\n--- Sample Overrated Outliers ---\")\n",
    "print(df[df[\"Is Overrated Outlier\"]][[\"True Rating\", \"Expected Rating\", \"Review Text\"]].head(3))\n",
    "\n",
    "print(\"\\n--- Sample Underrated Outliers ---\")\n",
    "print(df[df[\"Is Underrated Outlier\"]][[\"True Rating\", \"Expected Rating\", \"Review Text\"]].head(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
